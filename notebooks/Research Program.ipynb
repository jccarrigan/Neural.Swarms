{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Swarm of Neural Units\n",
    "**Goal:** develop a swarm intelligence model with agents which\n",
    "1. Can navigate a local space using internal knowledge\n",
    "2. Can update that knowledge using sensor data\n",
    "3. Can navigate a larger space using swarm knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Development Pathways**\n",
    "\n",
    "Acheiving all the goals above involves several interrelated development pathways. \n",
    "1. **Agent** \n",
    "development involves designing and training agents which can perform as desired in simulations.\n",
    "2. **Swarm** \n",
    "development involves testing groups of agents in simulation, and showing how the swarm performs against individual units.\n",
    "3. **Hardware** \n",
    "development entails preparing connectivity functions to collect data from hardware agents (Sphero robotic balls) and passing them into the swarm model for processing. To limit development requirements, the entire hardware swarm will be run virtually on a computer, and commands passed back to individual units. This will simulate each agent having its own intelligence without having to actually design a version of the software that is compatible with Sphero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development Plan\n",
    "**Simulation Stage**\n",
    "1. Design agent\n",
    "2. Test agent\n",
    "3. Test swarm of agents\n",
    "4. Repeat until goal #3 is acheived\n",
    "\n",
    "**Deployment Stage**\n",
    "1. Modify agent classes until they can communicate over Bluetooth to Sphero robotic balls\n",
    "2. Test swarm of Spheros in basic environments\n",
    "3. Two goals: \n",
    "    -individual navigation \n",
    "    -collective cohesion\n",
    "\n",
    "**Experimentation Stage**\n",
    "- In simulations, design more agent types and test swarms with different compositions\n",
    "- In Sphero Swarm, design more task tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This Document**\n",
    "\n",
    "This document records my research on swarms of units with neural network intelligence. Style guide\n",
    "1. each experiment description should have a reproducable simulation\n",
    "2. each conclusion should drive into the next experiment\n",
    "3. mathematical formulations should be well-formed\n",
    "4. break up discussion and computational cells\n",
    "5. break discussion cells by paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Swarm Basics**\n",
    "\n",
    "   For development, I focus on simulation. However, I may be able to deploy the project to a small swarm of Sphero robot balls. Deployment may or may not occur depending on how well development goes. I will use <a href=\"https://github.com/zdanowiczkonrad/PythonSphero\">PythonSphero</a> which implements the <a href=\"https://github.com/karol-szuster/kulka\">kulka</a> Sphero API. The swarm simulation is based on basic cellular automata. Rather than following formulated rules, behavior is governed by a neural network. I am using a base swarm class from github user <a href='https://github.com/elmar-hinz/'>elmar-hinz</a> called <a href='https://github.com/elmar-hinz/Python.Swarms'>Python.Swarms</a>. It was developed in Python 2.7 for visualization in a terminal class called curses. So, I have adapted it for Python 3 and display in Jupyter Notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 0: Navigation with stationary \"sensors\"\n",
    "- **Agent** Stationary \"sensor\" that can \"see\" a radius r- with the taxicab metric, mobile navigator.\n",
    "- **Quantitiy** 1 neural agent, 1 dumb agent. Since this test will be done with \"perfect\" sensors, no need to train every tower. Only the 'navigator' is trained.\n",
    "- **Training Method** Failed reinforcement on an MLP\n",
    "- **Notes** The pieces all worked together: game backend, neural net, and my visualizations. But, everything got bloated and convoluted (not as in the nn...) so moving on to another experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Navigation with a vector\n",
    "- **Agent** Navigator with an MLP policy evaluator. Input is a vector to target, output is an estimate of value of one of five actions. Navigator chooses highest value.\n",
    "- **Quantitiy** 1 neural agent, 2 => hidden => 5\n",
    "- **Training Method** Reinforcement/Q learning on an MLP\n",
    "- May 14, 2017 Notes: \n",
    "    - Pieces in place, can train a model on one goal at a time. \n",
    "    - Seems to kinda work? Very slowly. Now trying to train it a lot and see if I can get it moving faster.\n",
    "    - Realized we can move up to an RNN by feeding the rewards back into it. \n",
    "    - Also really need to implement a \"game over\" method, stop training on non-interesting states. \n",
    "    - But is that relevant when it trains on a random path?\n",
    "    - Was working with an input of (pos, goal), so that one model can play multiple games.\n",
    "    - To contextualize the whole \"game over\" idea, the game is not over until the agent learns what it's supposed to do.\n",
    "    - SO. The game is training the model. \n",
    "    - And now only training a network with (pos) for input\n",
    "    - I think I've just had my first major success! I must consider how to use this...\n",
    "    - The network succesfully trains with goals at (7, 7) and (3, 3) on a 16 by 16 px board.\n",
    "- Final Details\n",
    "    - Three-layer ReLu MLP with two input neurons (for position)\n",
    "    - 20-neuron hidden layer\n",
    "    - Five output neurons (one for each action/value) \n",
    "    - Convergence occured with 1000 game steps and 20 epochs \n",
    "    - using SGD and a learning rate 0.005.\n",
    "    - file: `navi_game.py`\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Navigation with position and goal\n",
    "- **Agent** Navigator with an MLP policy evaluator. Input is a vector to target, output is an estimate of value of one of five actions. Navigator chooses highest value. The main difference between this and the previous experiment is that the goal is an input to our neural network. So, the model will be trained and tested on with multiple flag locations.\n",
    "- **Quantitiy** 1 neural agent, 4 => hidden => hidden => 5\n",
    "- **Training Method** Reinforcement/Q learning on an MLP\n",
    "- May 14, 2017\n",
    "    - Using the training regime discovered above to train the model on multiple games\n",
    "    - With 100 games, the network had come close to convergence, and stopped at a point that was not the flag\n",
    "    - With 500 games, similar results. Validating performance, I found that the model still played the very first game fine, but subsetquent games were terrible. So, back to 100 games, but different training regimes for the later ones vs the first one. Or maybe more games, shorter runs...? Yeah the latter first.\n",
    "    - 5000 games, 100 steps each... ? Not much better\n",
    "    - Added a layer, now running 50k games with 1000 steps each, so 5 million data points. We will see in the morning!\n",
    "- May 15, 2017    \n",
    "    - So, that didn't work much better than some of the other shorter runs. Worth it, cuz it gave me a chance to sleep.\n",
    "    - As that run most likely contained every game of this variation (there are only 256) we can safely say that the number of games isn't improving performance. We need to train on the data differently, and maybe use a different optimizer. \n",
    "    - We'll use no more than 500 games (about two of each, 99% chance of having at least two thirds of the games)\n",
    "    - We'll shuffle merge their training logs into a single log, and train on that \n",
    "    - Slightly better results with the above, but still performing poorly on the \"final boss\" game.\n",
    "    - Go deeper?\n",
    "- May 16, 2017\n",
    "    - Still struggling to achieve positive results. Some parameters to think about:\n",
    "    - We are playing on a `16 x 16` board with only two pieces - the `Flag` and the `Navigator`\n",
    "    - This gives 256 possible games, with 256 possible states\n",
    "    - Each game is equally likely, and each initial state is equally likely\n",
    "    - Any given initial state of a given game is equally likely, but the initial state determines what internal states can be seen, since we generate them by randomly making allowed moves\n",
    "    -However, the reward generator is game-agnostic, meaning it can be called with a totally random `Navigator` and `Flag` position\n",
    "    - Perhaps that is how we should generate data?\n",
    "    - Also, avoid training on more than 100 games. As many states as desired from each game, but try to minimize the number of games so we can present the network with new games. It is critical that the agent be able to navigate to arbitrary positions, not just positions it trained to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Experiment: Mapping with stationary \"sensors\"\n",
    "- **Agent** Stationary \"sensor\" that can \"see\" a radius r- with the taxicab metric.\n",
    "- **Quantitiy** 1 neural agaent, n dumb agents. Since this test will be done with \"perfect\" sensors, no need to train every tower. Only the 'navigator' is trained.\n",
    "- **Training Method** Not sure yet, hopefully reinforcement on an RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Deliberate swarm mapoing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Noisy sensor simulations\n",
    "Repeat all of the above with \"noise\" methods implemented for all simulated sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Coordinated motion, maximize information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
